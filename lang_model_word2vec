from lang_model import LangModel

from gensim.models import Word2Vec
import pickle
import numpy as np
import os

from logging import getLogger, config
logger = getLogger(os.path.basename(__file__))


class LangModelWord2Vec(LangModel):

    def __init__(self, domain_name, stop_words=[], meaningless_words_reg=[], domain_common_words=[], remove_verb=False, abbs_dict=None, clear_cache=False) -> None:
        super().__init__(domain_name, stop_words, meaningless_words_reg, domain_common_words, remove_verb, abbs_dict, clear_cache)


    def get_lang_name(self):
        return 'word2vec'


    def learn_sub(self, data_for_model):
        return Word2Vec(data_for_model)

    def most_similar(self, w, topn=10):
        return self.model.wv.most_similar(w,topn)


    def save_model_sub(self, model_file):
        with open(model_file, 'wb') as f:
            pickle.dump(self.model, f)

    def load_model(self, model_file):
        with open(model_file, 'rb') as f:
            self.model = pickle.load(f)


    def get_word_vector(self, w):
        if w in self.model.wv.key_to_index.keys():
            return self.model.wv[w]
        else:
            pass


    def calc_dis1(self, ss1, ss2):

        num = 0
        val = 0

        wv = self.model.wv

        for s1 in ss1:
            # print(f's1 = {s1}')
            if not s1 in wv.key_to_index.keys():
                continue

            for s2 in ss2:
                # print(f's2 = {s2}')
                if not s2 in wv.key_to_index.keys():
                    continue

                val += self.cos_similarity(wv[s1], wv[s2])
                num += 1

        if num != 0:
            val /= num

        return val


    def calc_dis2(self, ss1, ss2):

        num = 0
        val = 0

        wv = self.model.wv

        for s1 in ss1:
            if not s1 in wv.key_to_index.keys():
                continue

            sub_val = 0
            for s2 in ss2:
                if not s2 in wv.key_to_index.keys():
                    continue

                tmp_val = self.cos_similarity(wv[s1], wv[s2])
                if sub_val < tmp_val:
                    sub_val = tmp_val
            
            val += sub_val
            num += 1

        if num != 0:
            val /= num

        val /= np.sqrt(np.abs(len(ss1) - len(ss2)) + 1)

        return val


    def calc_dis3(self, ss1, ss2):

        if ss1 == [] or ss2 == []:
            return 10

        val = self.model.wv.wmdistance(ss1, ss2)
        if np.isinf(val):
            val = 10

        return 10 - val


    def calc_based_on_sentence_vector(self, ss1, ss2):

        sss1 = []
        for s1 in ss1:
            if s1 in self.model.wv.key_to_index.keys():
                sss1.append(s1)

        sss2 = []
        for s2 in ss2:
            if s2 in self.model.wv.key_to_index.keys():
                sss2.append(s2)

        if sss1 == [] or sss2 == []:
            return 0

        v1 = self.our_get_sentence_vector(sss1)
        v2 = self.our_get_sentence_vector(sss2)

        return self.cos_similarity(v1, v2)


    def calc_score_sub(self, s1, s2, option=''):

        if option == '':
            return self.calc_based_on_sentence_vector(s1, s2)
        elif option == 'dis1':
            return self.calc_dis1(s1, s2)
        elif option == 'dis2':
            return self.calc_dis2(s1, s2)
        elif option == 'dis3':
            return self.calc_dis3(s1, s2)
        else:
            logger.error(f'unknown option: {option}')

    def most_similar(self, w, topn=10):
        return self.model.wv.most_similar(w,topn)

    def get_vector_size(self):
        return self.model.wv.vector_size

    def get_word_vector(self, word):
        return self.model.wv[word]
        



